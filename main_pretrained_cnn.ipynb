{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files\n",
    "from keras.utils import np_utils\n",
    "import numpy\n",
    "from glob import glob\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "keras = tf.keras\n",
    "SPLIT_WEIGHTS = (8, 1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#just declaring some UTILS functions\n",
    "IMG_SIZE = 150\n",
    "def format_example(image, label):\n",
    "  image = tf.cast(image, tf.float32)\n",
    "  image = (image/127.5) - 1\n",
    "  image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n",
    "  return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataset in splitted sizes\n",
    "raw_train,metadata = tfds.load(\"stanford_dogs\", split=\"train[:80%]\",with_info=True,as_supervised=True)\n",
    "raw_test,metadata = tfds.load(\"stanford_dogs\", split=\"train[81%:90%]\",with_info=True,as_supervised=True)\n",
    "raw_validation,metadata = tfds.load(\"stanford_dogs\", split=\"train[91%:]\",with_info=True,as_supervised=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check that the dataset has been loaded correctly\n",
    "get_label_name = metadata.features['label'].int2str\n",
    "for image, label in raw_train.take(2):\n",
    "  plt.figure()\n",
    "  plt.imshow(image)\n",
    "  plt.title(get_label_name(label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resize all images to 300\n",
    "train = raw_train.map(format_example)\n",
    "validation = raw_validation.map(format_example)\n",
    "test = raw_test.map(format_example)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffle and batch the data\n",
    "BATCH_SIZE = 32\n",
    "SHUFFLE_BUFFER_SIZE = 1000  \n",
    "\n",
    "train_batches = train.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "validation_batches = validation.batch(BATCH_SIZE)\n",
    "test_batches = test.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SHAPE = (IMG_SIZE, IMG_SIZE, 3)\n",
    "\n",
    "# Xception base model from github\n",
    "base_model = tf.keras.applications.Xception(input_shape=IMG_SHAPE,\n",
    "                                               include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_batch, label_batch in train_batches.take(1):\n",
    "  pass \n",
    "image_batch.shape\n",
    "feature_batch = base_model(image_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#declare global average layer pooling\n",
    "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "feature_batch_average = global_average_layer(feature_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare prediction layer on top\n",
    "prediction_layer = keras.layers.Dense(1)\n",
    "prediction_batch = prediction_layer(feature_batch_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile base model with global average pooling and prediction layer\n",
    "model = tf.keras.Sequential([base_model,global_average_layer,prediction_layer])\n",
    "#model = tf.keras.Sequential()\n",
    "#model.add(tf.keras.layers.BatchNormalization(input_shape=(224, 224, 3)))\n",
    "#model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=3, kernel_initializer='he_normal', activation='relu'))\n",
    "#model.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n",
    "#model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "#model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, kernel_initializer='he_normal', activation='relu'))\n",
    "#model.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n",
    "#model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "#model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=3, kernel_initializer='he_normal', activation='relu'))\n",
    "#model.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n",
    "#model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "#model.add(tf.keras.layers.Conv2D(filters=128, kernel_size=3, kernel_initializer='he_normal', activation='relu'))\n",
    "#model.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n",
    "#model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "#model.add(tf.keras.layers.Conv2D(filters=256, kernel_size=3, kernel_initializer='he_normal', activation='relu'))\n",
    "#model.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n",
    "#model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "#model.add(tf.keras.layers.GlobalAveragePooling2D())\n",
    "#model.add(tf.keras.layers.Flatten())\n",
    "#model.add(tf.keras.layers.Dense(133, activation='softmax'))\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the model\n",
    "base_learning_rate = 0.0001\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=base_learning_rate),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verify the model and trainable parameters\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = (\n",
    "  metadata.splits['train'].num_examples*8/10 \n",
    ")\n",
    "print(num_train)\n",
    "num_val = (\n",
    "  metadata.splits['train'].num_examples*1/10 \n",
    ")\n",
    "print(num_val)\n",
    "\n",
    "num_test  = (\n",
    "  metadata.splits['train'].num_examples*1/10 \n",
    ")\n",
    "print(num_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch = 10\n",
    "validation_steps = 20\n",
    "loss0,accuracy0 = model.evaluate(test_batches, steps = validation_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"initial loss: {:.2f}\".format(loss0))\n",
    "\n",
    "print(\"initial accuracy: {:.2f}\".format(accuracy0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_epochs = 10\n",
    "\n",
    "checkpointer = tf.keras.callbacks.ModelCheckpoint(filepath='saved_models/weights.bestaugmented.from_scratch.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "callbacks_list = [checkpointer]\n",
    "\n",
    "history = model.fit(train_batches,\n",
    "                    epochs=initial_epochs,\n",
    "                    callbacks=callbacks_list,\n",
    "                    validation_data=validation_batches,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('saved_models/weights.bestaugmented.from_scratch.hdf5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_batches,\n",
    "                    epochs=initial_epochs,\n",
    "                    callbacks=callbacks_list,\n",
    "                    validation_data=validation_batches,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_batches,\n",
    "                    epochs=initial_epochs,\n",
    "                    callbacks=callbacks_list,\n",
    "                    validation_data=validation_batches,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([min(plt.ylim()),1])\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.ylim([0,1.0])\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of layers in the base model: \", len(base_model.layers))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tune_at = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in base_model.layers[:fine_tune_at]:\n",
    "  layer.trainable =  False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer = tf.keras.optimizers.Adam(lr=base_learning_rate/10),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model.trainable_variables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tune_epochs = 10\n",
    "total_epochs =  initial_epochs + fine_tune_epochs\n",
    "\n",
    "history_fine = model.fit(train_batches,\n",
    "                         epochs=total_epochs,\n",
    "                         initial_epoch =  history.epoch[-1],\n",
    "                         validation_data=validation_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history_fine.history['accuracy']\n",
    "val_acc = history_fine.history['val_accuracy']\n",
    "\n",
    "loss = history_fine.history['loss']\n",
    "val_loss = history_fine.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([min(plt.ylim()),1])\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.ylim([0,1.0])\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
